lm_eval value



# previous bash command that seemed to work, except for the max tokens issue:

(venv) ubuntu@104-171-203-130:~/cs4nlp/eval/lm-evaluation-harness$ \
lm_eval --model vllm --model_args arallel_size=2,max_model_len=2048,gpu_memory_utilization=0.85 \
--tasks aime24_nofigures,openai_math --ts --log_samples --gen_kwargs \
"max_gen_toks=4096,max_tokens_thinking=auto"


# connect to cluster

    # Basic SSH command format
    ssh ubuntu@<IP-ADDRESS> -i <PATH-TO-YOUR-SSH-KEY>
    # example
    #ssh ubuntu@104.171.203.117 -i ./CS4NLP_GPUs.pem
    ssh ubuntu@104.171.202.48 -i ./CS4NLP_GPUs.pem
    ssh ubuntu@104.171.202.166 -i ./CS4NLP_GPUs.pem


    # generate key 
    ssh-keygen -t ed25519 -C "your_email@example.com"

    # copy key 
    cat ~/.ssh/id_ed25519.pub

    # store it to github keys 
    

    # test connection
    ssh -T git@github.com

    # From local machine, securely copy your private key to the cluster
    scp -i C:\Users\IT\Desktop\CS4NLP\CS4NLP_GPUs.pem ~/.ssh/id_ed25519 ubuntu@CLUSTER_IP:~/.ssh/
    scp -i C:\Users\IT\Desktop\CS4NLP\CS4NLP_GPUs.pem ~/.ssh/id_ed25519.pub ubuntu@CLUSTER_IP:~/.ssh/

    # example with cluster IP
    scp -i C:\Users\IT\Desktop\CS4NLP\CS4NLP_GPUs.pem ~/.ssh/id_ed25519 ubuntu@104.171.203.117:~/.ssh/
    scp -i C:\Users\IT\Desktop\CS4NLP\CS4NLP_GPUs.pem ~/.ssh/id_ed25519.pub ubuntu@104.171.203.117:~/.ssh/

    # on the cluster, set proper permissions
    ssh -i /path/to/lambda_labs_key.pem ubuntu@CLUSTER_IP "chmod 600 ~/.ssh/id_ed25519"




# standard way to launch it 
-> double check

  # commands to connect to cluster 
  # 1. Clone repo
  git clone git@github.com:alexander-brady/cs4nlp.git
  cd cs4nlp

  # 2. Set up a venv
  python -m venv venv
  source venv/bin/activate

  # in case in a branch
  git branch
  git checkout feature/edging_it

  # 3. Install dependencies
  cd eval/lm-evaluation-harness
  pip install -e .
  pip install vllm torch

  # add processor and API key
  export PROCESSOR="gpt-4o-mini"
  export OPENAI_API_KEY=""

  # 4. Run the evaluation with single/multi GPU command
  ... below 



lm_eval --model vllm --model_args pretrained=simplescaling/s1.1-1.5B,dtype=float16 \
  --tasks aime24_nofigures --batch_size auto --apply_chat_template \
  --output_path results --gen_kwargs "max_gen_toks=32768,thinking_n_ignore_str=Wait"


## lm eval with list of tokens

# for single GPU

lm_eval --model vllm \
  --model_args pretrained=simplescaling/s1.1-1.5B,max_model_len=2048,gpu_memory_utilization=0.85,\
  thinking_n_ignore_str="Wait;Let\'s think;Hmm;I need to analyze;Let me process" \
  --tasks aime24_nofigures,openai_math \
  --batch_size auto \
  --log_samples \
  --gen_kwargs max_gen_toks=2000,max_tokens_thinking=1800


# for multiple GPUs


# newer version 
lm_eval --model vllm \
  --model_args "pretrained=simplescaling/s1.1-1.5B,tensor_parallel_size=4,max_model_len=2048,gpu_memory_utilization=0.85" \
  --tasks gsm8k,aime24_nofigures,openai_math \
  --batch_size auto \
  --log_samples \
  --output_path ./results \
  --gen_kwargs "max_gen_toks=32768,max_tokens_thinking=1800,thinking_n_ignore_str=\"Wait###Let us perhaps reconsider...###Hmm I should double-check this.###Let me verify this calculation.###I need to analyze this from another angle.###Let me work through this step-by-step.###I am not confident in this result yet.###Let me trace through the logic again.###There may be something I am overlooking here.###This deserves more careful analysis.###Let me reason through this systematically.###Is this even right?###Is this correct###Could this be wrong###So given this and that###Is this the right conclusion?###Is this sound?###Is this a logical conclusion...?\""

# more budget forcing
lm_eval --model vllm \
  --model_args "pretrained=simplescaling/s1.1-1.5B,tensor_parallel_size=4,gpu_memory_utilization=0.85" \
  --tasks gsm8k,aime24_nofigures,openai_math \
  --batch_size auto \
  --log_samples \
  --output_path ./results \
  --gen_kwargs "max_gen_toks=32768,max_tokens_thinking=auto,thinking_n_ignore_str=\"Wait###Hmm I should double-check this.###Do we need to think more?###Let me verify this calculation.###I need to analyze this from another angle.###Let me work through this step-by-step.###Am I confident in this result?###Let me trace through the logic again.###This deserves more careful analysis.###Let me reason through this systematically.######Is this correct###Could this be wrong###So given this and that###Is this the right conclusion?###Is this sound?###Is this a logical conclusion...?\""




lm_eval --model vllm \
  --model_args "pretrained=simplescaling/s1.1-1.5B,tensor_parallel_size=4,max_model_len=2048,gpu_memory_utilization=0.85" \
  --tasks gsm8k,aime24_nofigures,openai_math \
  --batch_size auto \
  --log_samples \
  --output_path ./results \
  --gen_kwargs "max_gen_toks=2000,max_tokens_thinking=1800,thinking_n_ignore_str=\"Wait need to make sure whether this is right.\""


# 24.05 single GPU deployment

lm_eval --model vllm \
  --model_args "pretrained=simplescaling/s1.1-1.5B,tensor_parallel_size=1,gpu_memory_utilization=0.85" \
  --tasks aime24_nofigures,openai_math \
  --batch_size auto \
  --log_samples \
  --output_path ./results \
  --gen_kwargs "max_gen_toks=32768,max_tokens_thinking=auto,thinking_n_ignore_str=\"Wait###Hmm I should double-check this.###Do we need to think more?###Let me verify this calculation.###I need to analyze this from another angle.###Let me work through this step-by-step.###Am I confident in this result?###Let me trace through the logic again.###This deserves more careful analysis.###Let me reason through this systematically.###Is this correct like this?###Could this be wrong###So given this and that###Is this the right conclusion?###Is this sound?###Is this a logical conclusion...?\""



## List of thinking tokens:
  
  "Wait;Wait ;Let us reconsider...;Hmm, I should double-check this.;On second thought...;\
    Actually...;Wait, that can not be right.;Let me verify this calculation.;\
    I need to analyze this from another angle.;Let us try a different approach.;\
    I should break this down further.;\Let me work through this step-by-step.;\
    I am not confident in this result yet.;\Let me trace through the logic again.;\
    There may be something I am overlooking here.;\This deserves more careful analysis.;\
    Let me reason through this systematically.;\
    Is this even right?; Is this correct; Could this be wrong; So, given this and that;\
    Is this the right conclusion?; Is this sound?; Is this a logical conclusion...;"\



# git commands
